akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = DEBUG
  logger-startup-timeout = 10s
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = 0
  log-dead-letters-during-shutdown = false

  cluster.jmx.multi-mbeans-in-same-jvm=on

  actor {

    warn-about-java-serializer-usage = on
    allow-java-serialization = off

    provider = cluster

    //Normally, messages sent between local actors (i.e. same JVM) do not undergo serialization. For testing, sometimes, it may be desirable to force serialization on all messages (both remote and local). If you want to do this in order to verify that your messages are serializable you can enable the following config option:
    #serialize-messages = on

    serializers {
      #serializer = "com.dsim.rdelivery.serialization.JobSerializer"
      #serializer = "akka.DirectMemorySerializer"

      #reliable-delivery = "akka.cluster.typed.internal.delivery.ReliableDeliverySerializer"
      reliable-delivery = "akka.DataSerializer"
    }

    serialization-identifiers {
      #"akka.cluster.typed.internal.delivery.ReliableDeliverySerializer" = 36
      "akka.DataSerializer" = 36
    }

    serialization-bindings {
      #"com.safechat.actors.UserJoined"        = journalSerializer

      "com.dsim.rdelivery.Master$JobDescription" = reliable-delivery
      "com.dsim.rdelivery.Worker$WorkerJob"      = reliable-delivery

      "akka.actor.typed.delivery.internal.DeliverySerializable" = reliable-delivery
    }

    default-dispatcher {
      fork-join-executor {
        parallelism-factor = 1.0
        parallelism-min = 2
        parallelism-max = 4
      }
    }
  }

  remote {
    artery {

      canonical.hostname = "127.0.0.1"

      # To notice large messages you can enable logging of message types with payload size in bytes larger than the configured
      log-frame-size-exceeding = 30 KiB #10000 b

      advanced {

        maximum-frame-size = 32 KiB

        #https://discuss.lightbend.com/t/how-to-avoid-nodes-to-be-quarantined-in-akka-cluster/1932/2

        #That should be rare, but for example if many actors stop at the same time and there are watchers of these actors on other nodes there
        # may be a storm of Terminated messages sent more quickly than they can be delivered and thereby filling up buffers.

        # This setting defines the maximum number of unacknowledged system messages
        # allowed for a remote system. If this limit is reached the remote system is
        # declared to be dead and its UID marked as quarantined.
        system-message-buffer-size = 20000 #The buffer is an ArrayDeque so it grows as needed, but doesnâ€™t shrink.

        outbound-control-queue-size = 20000
      }

      large-message-destinations = [
        # This setting disables log-frame-size-exceeding and maximum-frame-size so you won't see
        # akka.remote.OversizedPayloadException: Discarding oversized payload sent to Some(Actor[akka://dsim@127.0.0.1:2552/user/worker/consumer-controller#-941039105]): max allowed size 32768 bytes. Message type [akka.actor.typed.delivery.ConsumerController$SequencedMessage].
        "/user/worker/consumer-controller"

        #"/system/singletonManagermaster/master/producer-controller"
      ]
    }
  }

  cluster {
    configuration-compatibility-check.enforce-on-join = on #off

    # How many members are needed to form a cluster.
    #min-nr-of-members = 2

    seed-nodes = [
      "akka://dsim@127.0.0.1:2551"
      "akka://dsim@127.0.0.1:2552"
    ]

    #failure-detector {
    #  implementation-class = "akka.remote.PhiAccrualFailureDetector"
    #  threshold = 10 # 8
    #  heartbeat-interval = 1 s
    #  acceptable-heartbeat-pause = 4 s #3
    #}

    #use-dispatcher = akka.cluster-dispatcher

    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"

    split-brain-resolver {
      # static-quorum, keep-majority, keep-oldest, down-all, lease-majority
      active-strategy = keep-oldest

      #//#stable-after
      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # Decision is taken by the strategy when there has been no membership or
      # reachability changes for this duration, i.e. the cluster state is stable.
      stable-after = 7s

      # When reachability observations by the failure detector are changed the SBR decisions
      # are deferred until there are no changes within the 'stable-after' duration.
      # If this continues for too long it might be an indication of an unstable system/network
      # and it could result in delayed or conflicting decisions on separate sides of a network
      # partition.
      # As a precaution for that scenario all nodes are downed if no decision is made within
      # `stable-after + down-all-when-unstable` from the first unreachability event.
      # The measurement is reset if all unreachable have been healed, downed or removed, or
      # if there are no changes within `stable-after * 2`.
      # The value can be on, off, or a duration.
      # By default it is 'on' and then it is derived to be 3/4 of stable-after.
      down-all-when-unstable = on

    }
    
    #OldestAutoDowning automatically downs unreachable members. A node responsible to down is the oldest member of a specified role.
    # If oldest-member-role is not specified, the oldest member among all cluster members fulfills its duty.
    #Downsides: all but one go down when the oldest is separated from the rest of the cluster
    #downing-provider-class = "org.sisioh.akka.cluster.custom.downing.OldestAutoDowning"

    #custom-downing {
      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # Decision is taken by the strategy when there has been no membership or
      # reachability changes for this duration, i.e. the cluster state is stable.

      #stable-after = 5s

      #oldest-auto-downing {
        #oldest-member-role = ""

        #Downside of the oldest based downing strategy is loss of downing functionality when the oldest member itself fails.
        #If down-if-alone is set to be true, such scenario can be avoided because the secondary oldest member will down the oldest member
        #if the oldest member get unreachable alone.

        #down-if-alone = true
      #}
    #}
  }

  # CoordinatedShutdown will run the tasks that are added to these
  # phases. The phases can be ordered as a DAG by defining the
  # dependencies between the phases.
  # Each phase is defined as a named config section with the
  # following optional properties:
  # - timeout=15s: Override the default-phase-timeout for this phase.
  # - recover=off: If the phase fails the shutdown is aborted
  #                and depending phases will not be executed.
  # depends-on=[]: Run the phase after the given phases
  coordinated-shutdown {
    # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
    # if this is set to 'on'. It is done after termination of the
    # ActorSystem if terminate-actor-system=on, otherwise it is done
    # immediately when the last phase is reached.
    exit-jvm = on

    # Run the coordinated shutdown when ActorSystem.terminate is called.
    # Enabling this and disabling terminate-actor-system is not a supported
    # combination (will throw ConfigurationException at startup).
    run-by-actor-system-terminate = on


    default-phase-timeout = 10 seconds
  }

  #persistence {
  #  journal.plugin = "cassandra-journal"
  #  snapshot-store.plugin = "cassandra-snapshot-store"
  #}

  persistence {

    #https://doc.akka.io/docs/akka/current/typed/persistence.html#replay-filter
    #https://blog.softwaremill.com/akka-cluster-split-brain-failures-are-you-ready-for-it-d9406b97e099
    #So if a split brain produces more wrong events than window-size then your aggregate state will be corrupted
    journal-plugin-fallback.replay-filter.window-size = 150 #100

    journal {
      plugin = ${akka.persistence.cassandra.journal}
      auto-start-journals = [ akka.persistence.cassandra.journal ]
    }

    snapshot-store {
      plugin = ${akka.persistence.cassandra.snapshot}
      auto-start-snapshot-stores = [ akka.persistence.cassandra.snapshot ]
    }

    cassandra {

      journal {

        # don't do this in production, convenient for local example
        keyspace-autocreate = true
        tables-autocreate = true

        keyspace = "msg"
        table = "msg_journal"

        # Maximum number of messages that will be batched when using `persistAsync`.
        # Also used as the max batch size for deletes.
        max-message-batch-size = 200

        # Target number of entries per partition (= columns per row).
        # Must not be changed after table creation (currently not checked).
        # This is "target" as AtomicWrites that span partition boundaries will result in bigger partitions to ensure atomicity.
        target-partition-size = 4096

        replication-factor = 1 #3

        support-all-persistence-ids = off
      }

      events-by-tag {
        # Enable/disable events by tag. If eventsByTag queries aren't required then this should be set to
        # false to avoid the overhead of maintaining the tag_views table.
        enabled = false
      }

      snapshot {

        # don't do this in production, convenient for local example
        keyspace-autocreate = true
        tables-autocreate = true

        keyspace = "msg_snapshot"
        table = "msg_snapshots_journal"

        replication-factor = 1 #3
      }
    }
  }
}

akka.reliable-delivery {

  producer-controller {

    event-sourced-durable-queue {
      # Max duration for the exponential backoff for persist failures.
      restart-max-backoff = 10s

      # Snapshot after this number of events. See RetentionCriteria.
      snapshot-every = ${akka.persistence.cassandra.journal.target-partition-size}

      # Number of snapshots to keep. See RetentionCriteria.
      keep-n-snapshots = 2

      # Delete events after snapshotting. See RetentionCriteria.
      delete-events = on

      # Cleanup entries that haven't be used for this duration.
      cleanup-unused-after = 3600 s

      # The journal plugin to use, by default it will use the plugin configured by `akka.persistence.journal.plugin`.
      journal-plugin-id = akka.persistence.cassandra.journal

      # The journal plugin to use, by default it will use the plugin configured by `akka.persistence.snapshot-store.plugin`.
      snapshot-plugin-id =  akka.persistence.cassandra.snapshot
    }

    durable-queue {
      # The ProducerController uses this timeout for the requests to
      # the durable queue. If there is no reply within the timeout it
      # will be retried.
      request-timeout = 3 s

      # The ProducerController retries requests to the durable queue this
      # number of times before failing.
      retry-attempts = 10

      # The ProducerController retries sending the first message with this interval
      # until it has been confirmed.
      resend-first-interval = 1 s
    }

    # To avoid head of line blocking from serialization and transfer
    # of large messages this can be enabled.
    # Large messages are chunked into pieces of the given size in bytes. The
    # chunked messages are sent separatetely and assembled on the consumer side.
    # Serialization and deserialization is performed by the ProducerController and
    # ConsumerController respectively instead of in the remote transport layer.

    # if (producerControllerSettings.chunkLargeMessagesBytes > 0) throw new IllegalArgumentException("Chunked messages not implemented for work-pulling yet.")
    #chunk-large-messages = 120 KiB

  }

  consumer-controller {
    # Number of messages in flight between ProducerController and
    # ConsumerController. The ConsumerController requests for more messages
    # when half of the window has been used.
    flow-control-window = 16

    # The ConsumerController resends flow control messages to the
    # ProducerController with the resend-interval-min, and increasing
    # it gradually to resend-interval-max when idle.
    resend-interval-min = 1s
    resend-interval-max = 3s

    # If this is enabled lost messages will not be resent, but flow control is used.
    # This can be more efficient since messages don't have to be
    # kept in memory in the `ProducerController` until they have been
    # confirmed, but the drawback is that lost messages will not be delivered.
    only-flow-control = false
  }

  work-pulling {
    producer-controller = ${akka.reliable-delivery.producer-controller}
    producer-controller {
      # Limit of how many messages that can be buffered when there
      # is no demand from the consumer side.
      buffer-size = 16

      # Ask timeout for sending message to worker until receiving Ack from worker
      internal-ask-timeout = 5 #60s
    }
  }
}

datastax-java-driver {

  advanced {

    reconnect-on-init = true

    #auth-provider {
    #  class = PlainTextAuthProvider
    #  username = ...
    #  password = ...
    #}
  }

  basic {
    #contact-points = [""]
    load-balancing-policy.local-datacenter = "datacenter1"
  }

  profiles {
    akka-persistence-cassandra-profile {
      basic.request {
        #only for development
        consistency = ONE #QUORUM
      }
    }

    akka-persistence-cassandra-snapshot-profile {
      basic.request {
        #only for development
        consistency = ONE #QUORUM
      }
    }
  }
}


pinned-dispatcher {
  type = PinnedDispatcher
  executor = "thread-pool-executor"
}

fj-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
      parallelism-min = 2
      parallelism-max = 8
  }
}
