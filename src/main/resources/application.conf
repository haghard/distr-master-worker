akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = DEBUG
  logger-startup-timeout = 10s
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = 0
  log-dead-letters-during-shutdown = false

  cluster.jmx.multi-mbeans-in-same-jvm=on

  actor {
    warn-about-java-serializer-usage=off

    allow-java-serialization = on

    provider = cluster

    default-dispatcher {
      fork-join-executor {
        parallelism-factor = 1.0
        parallelism-min = 2
        parallelism-max = 4
      }
    }
  }

  remote {
    artery {
      canonical.hostname = "127.0.0.1"
      advanced {
        #maximum-frame-size = 2MiB


        #https://discuss.lightbend.com/t/how-to-avoid-nodes-to-be-quarantined-in-akka-cluster/1932/2

        #That should be rare, but for example if many actors stop at the same time and there are watchers of these actors on other nodes there
        # may be a storm of Terminated messages sent more quickly than they can be delivered and thereby filling up buffers.

        # This setting defines the maximum number of unacknowledged system messages
        # allowed for a remote system. If this limit is reached the remote system is
        # declared to be dead and its UID marked as quarantined.
        system-message-buffer-size = 20000 #The buffer is an ArrayDeque so it grows as needed, but doesnâ€™t shrink.

        outbound-control-queue-size = 20000
      }
    }
  }

  cluster {
    configuration-compatibility-check.enforce-on-join = off
    # How many members are needed to form a cluster.
    min-nr-of-members = 2

    seed-nodes = [
      "akka://dsim@127.0.0.1:2551"
      "akka://dsim@127.0.0.1:2552"
    ]

    #failure-detector {
    #  implementation-class = "akka.remote.PhiAccrualFailureDetector"
    #  threshold = 10 # 8
    #  heartbeat-interval = 1 s
    #  acceptable-heartbeat-pause = 4 s #3
    #}

    #use-dispatcher = akka.cluster-dispatcher

    #OldestAutoDowning automatically downs unreachable members. A node responsible to down is the oldest member of a specified role.
    # If oldest-member-role is not specified, the oldest member among all cluster members fulfills its duty.
    #Downsides: all but one go down when the oldest is separated from the rest of the cluster
    #downing-provider-class = "org.sisioh.akka.cluster.custom.downing.OldestAutoDowning"

    custom-downing {

      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # Decision is taken by the strategy when there has been no membership or
      # reachability changes for this duration, i.e. the cluster state is stable.
      stable-after = 5s

      oldest-auto-downing {
        oldest-member-role = ""
        #Downside of the oldest based downing strategy is loss of downing functionality when the oldest member itself fails.
        #If down-if-alone is set to be true, such scenario can be avoided because the secondary oldest member will down the oldest member
        #if the oldest member get unreachable alone.
        down-if-alone = true
      }
    }
  }

  # CoordinatedShutdown will run the tasks that are added to these
  # phases. The phases can be ordered as a DAG by defining the
  # dependencies between the phases.
  # Each phase is defined as a named config section with the
  # following optional properties:
  # - timeout=15s: Override the default-phase-timeout for this phase.
  # - recover=off: If the phase fails the shutdown is aborted
  #                and depending phases will not be executed.
  # depends-on=[]: Run the phase after the given phases
  coordinated-shutdown {
    # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
    # if this is set to 'on'. It is done after termination of the
    # ActorSystem if terminate-actor-system=on, otherwise it is done
    # immediately when the last phase is reached.
    exit-jvm = on

    # Run the coordinated shutdown when ActorSystem.terminate is called.
    # Enabling this and disabling terminate-actor-system is not a supported
    # combination (will throw ConfigurationException at startup).
    run-by-actor-system-terminate = on


    default-phase-timeout = 10 seconds
  }
}


akka.reliable-delivery {
  producer-controller {
    durable-queue {
      # The ProducerController uses this timeout for the requests to
      # the durable queue. If there is no reply within the timeout it
      # will be retried.
      request-timeout = 3s

      # The ProducerController retries requests to the durable queue this
      # number of times before failing.
      retry-attempts = 10

      # The ProducerController retries sending the first message with this interval
      # until it has been confirmed.
      resend-first-interval = 1s
    }
  }

  consumer-controller {
    # Number of messages in flight between ProducerController and
    # ConsumerController. The ConsumerController requests for more messages
    # when half of the window has been used.
    flow-control-window = 16

    # The ConsumerController resends flow control messages to the
    # ProducerController with the resend-interval-min, and increasing
    # it gradually to resend-interval-max when idle.
    resend-interval-min = 2s
    resend-interval-max = 30s

    # If this is enabled lost messages will not be resent, but flow control is used.
    # This can be more efficient since messages don't have to be
    # kept in memory in the `ProducerController` until they have been
    # confirmed, but the drawback is that lost messages will not be delivered.
    only-flow-control = false
  }

  work-pulling {
    producer-controller = ${akka.reliable-delivery.producer-controller}
    producer-controller {
      # Limit of how many messages that can be buffered when there
      # is no demand from the consumer side.
      buffer-size = 16

      # Ask timeout for sending message to worker until receiving Ack from worker
      internal-ask-timeout = 60s
    }
  }
}