akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = DEBUG
  logger-startup-timeout = 10s
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = 0
  log-dead-letters-during-shutdown = false

  cluster.jmx.multi-mbeans-in-same-jvm = on

  actor {

    warn-about-java-serializer-usage = on
    allow-java-serialization = off

    provider = cluster

    //Normally, messages sent between local actors (i.e. same JVM) do not undergo serialization.
    //For testing though, it may be desirable to force serialization on all messages (both remote and local).
    //If you want to do this in order to verify that your messages are serializable you can enable it
    #serialize-messages = on


    //https://github.com/rockthejvm/akka-serialization/blob/master/src/main/resources/protobufSerialization.conf
    serializers {
      #serializer = "com.dsim.rdelivery.serialization.JobSerializer"
      #serializer = "akka.DirectMemorySerializer"

      #reliable-delivery = "akka.cluster.typed.internal.delivery.ReliableDeliverySerializer"
      #reliable-delivery = "akka.ReliableDeliverySerializer2"

      proto = "akka.remote.serialization.ProtobufSerializer"

      #1
      reliable-delivery = "akka.ReliableDeliveryDirectSerializer2"
      #reliable-delivery = "akka.cluster.typed.internal.delivery.ReliableDeliverySerializer"
    }

    serialization-bindings {
      #"com.dsim.domain.v1.WorkerTaskPB" = proto
      #"scalapb.GeneratedMessage" = proto
      #"com.google.protobuf.Message" = proto

      #2
      "akka.actor.typed.delivery.internal.DeliverySerializable" = reliable-delivery
    }

    serialization-identifiers {
      "akka.remote.serialization.ProtobufSerializer" = 2

      #"akka.cluster.typed.internal.delivery.ReliableDeliverySerializer" = 36
      #"akka.ReliableDeliverySerializer2" = 36

      #3
      "akka.ReliableDeliveryDirectSerializer2" = 36
    }

    default-dispatcher {
      fork-join-executor {
        parallelism-factor = 1.0
        parallelism-min = 2
        parallelism-max = 4
      }
    }
  }

  remote {

    # https://doc.akka.io/docs/akka/current/remoting-artery.html#bytebuffer-based-serialization
    artery {
      transport = aeron-udp

      large-message-destinations = [
        # This setting disables log-frame-size-exceeding and maximum-frame-size so you won't see
        # akka.remote.OversizedPayloadException: Discarding oversized payload sent to Some(Actor[akka://dsim@127.0.0.1:2552/user/worker/consumer-controller#-941039105]): max allowed size 32768 bytes. Message type [akka.actor.typed.delivery.ConsumerController$SequencedMessage].
        "/user/worker/consumer-controller"

        #"/system/singletonManagermaster/master/producer-controller"
      ]

      # To notice large messages you can enable logging of message types with payload size in bytes larger than the configured
      log-frame-size-exceeding = 230 KiB #10000 b

      advanced {
        # Maximum serialized message size, including header data.
        maximum-frame-size = 250 KiB


        # Direct byte buffers are reused in a pool with this maximum size.
        # Each buffer has the size of 'maximum-frame-size'.
        # This is not a hard upper limit on number of created buffers. Additional
        # buffers will be created if needed, e.g. when using many outbound
        # associations at the same time. Such additional buffers will be garbage
        # collected, which is not as efficient as reusing buffers in the pool.
        buffer-pool-size = 128

        # Maximum serialized message size for the large messages, including header data.
        # It is currently restricted to 1/8th the size of a term buffer that can be
        # configured by setting the 'aeron.term.buffer.length' system property.
        # See 'large-message-destinations'.
        maximum-large-frame-size = 2 MiB

        # Direct byte buffers for the large messages are reused in a pool with this maximum size.
        # Each buffer has the size of 'maximum-large-frame-size'.
        # See 'large-message-destinations'.
        # This is not a hard upper limit on number of created buffers. Additional
        # buffers will be created if needed, e.g. when using many outbound
        # associations at the same time. Such additional buffers will be garbage
        # collected, which is not as efficient as reusing buffers in the pool.
        large-buffer-pool-size = 32

        # Total number of inbound lanes, shared among all inbound associations. A value
        # greater than 1 means that deserialization can be performed in parallel for
        # different destination actors. The selection of lane is based on consistent
        # hashing of the recipient ActorRef to preserve message ordering per receiver.
        # Lowest latency can be achieved with inbound-lanes=1 because of one less
        # asynchronous boundary.
        inbound-lanes = 4

        # Number of outbound lanes for each outbound association. A value greater than 1
        # means that serialization and other work can be performed in parallel for different
        # destination actors. The selection of lane is based on consistent hashing of the
        # recipient ActorRef to preserve message ordering per receiver. Note that messages
        # for different destination systems (hosts) are handled by different streams also
        # when outbound-lanes=1. Lowest latency can be achieved with outbound-lanes=1
        # because of one less asynchronous boundary.
        outbound-lanes = 1

        # This setting defines the maximum number of unacknowledged system messages
        # allowed for a remote system. If this limit is reached the remote system is
        # declared to be dead and its UID marked as quarantined.
        system-message-buffer-size = 20000 #The buffer is an ArrayDeque so it grows as needed, but doesnâ€™t shrink.

        # Size of the send queue for outgoing control messages, such as system messages.
        # If this limit is reached the remote system is declared to be dead and its UID
        # marked as quarantined. Note that there is one such queue per outbound association.
        # It is a linked queue so it will not use more memory than needed but by increasing
        # too much you may risk OutOfMemoryError in the worst case.
        outbound-control-queue-size = 20000

        # Only used when transport is aeron-udp
        aeron {

          # Periodically log out all Aeron counters. See https://github.com/real-logic/aeron/wiki/Monitoring-and-Debugging#counters
          # Only used when transport is aeron-udp.
          log-aeron-counters = false

          # Controls whether to start the Aeron media driver in the same JVM or use external
          # process. Set to 'off' when using external media driver, and then also set the
          # 'aeron-dir'.
          # Only used when transport is aeron-udp.
          embedded-media-driver = on

          # Directory used by the Aeron media driver. It's mandatory to define the 'aeron-dir'
          # if using external media driver, i.e. when 'embedded-media-driver = off'.
          # Embedded media driver will use a this directory, or a temporary directory if this
          # property is not defined (empty).
          # Only used when transport is aeron-udp.
          #aeron-dir = ""

          # Whether to delete aeron embedded driver directory upon driver stop.
          # Only used when transport is aeron-udp.
          delete-aeron-dir = yes

          # Level of CPU time used, on a scale between 1 and 10, during backoff/idle.
          # The tradeoff is that to have low latency more CPU time must be used to be
          # able to react quickly on incoming messages or send as fast as possible after
          # backoff backpressure.
          # Level 1 strongly prefer low CPU consumption over low latency.
          # Level 10 strongly prefer low latency over low CPU consumption.
          # Only used when transport is aeron-udp.
          idle-cpu-level = 1

          # messages that are not accepted by Aeron are dropped after retrying for this period
          # Only used when transport is aeron-udp.
          give-up-message-after = 60 seconds

          # Timeout after which aeron driver has not had keepalive messages
          # from a client before it considers the client dead.
          # Only used when transport is aeron-udp.
          client-liveness-timeout = 20 seconds

          # Timout after after which an uncommitted publication will be unblocked
          # Only used when transport is aeron-udp.
          publication-unblock-timeout = 40 seconds

          # Timeout for each the INACTIVE and LINGER stages an aeron image
          # will be retained for when it is no longer referenced.
          # This timeout must be less than the 'handshake-timeout'.
          # Only used when transport is aeron-udp.
          image-liveness-timeout = 10 seconds

          # Timeout after which the aeron driver is considered dead
          # if it does not update its C'n'C timestamp.
          # Only used when transport is aeron-udp.
          driver-timeout = 20 seconds
        }
      }
    }
  }

  cluster {
    configuration-compatibility-check.enforce-on-join = on #off

    # How many members are needed to form a cluster.
    #min-nr-of-members = 2

    #seed-nodes = [ "akka://dsim@127.0.0.1:2551", "akka://dsim@127.0.0.1:2552" ]

    #failure-detector {
    #  implementation-class = "akka.remote.PhiAccrualFailureDetector"
    #  threshold = 10 # 8
    #  heartbeat-interval = 1 s
    #  acceptable-heartbeat-pause = 4 s #3
    #}

    #use-dispatcher = akka.cluster-dispatcher

    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"

    split-brain-resolver {
      # static-quorum, keep-majority, keep-oldest, down-all, lease-majority


      # Keep the part that can acquire the lease, and down the other part.
      # Best effort is to keep the side that has most nodes, i.e. the majority side.
      # This is achieved by adding a delay before trying to acquire the lease on the
      # minority side.

      active-strategy = keep-oldest

      #active-strategy = lease-majority
      lease-majority {
        lease-implementation = "akka.coordination.lease.cassandra"

        # This delay is used on the minority side before trying to acquire the lease,
        # as an best effort to try to keep the majority side.
        acquire-lease-delay-for-minority = 2s
      }


      #//#stable-after
      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # Decision is taken by the strategy when there has been no membership or
      # reachability changes for this duration, i.e. the cluster state is stable.
      stable-after = 7s

      # When reachability observations by the failure detector are changed the SBR decisions
      # are deferred until there are no changes within the 'stable-after' duration.
      # If this continues for too long it might be an indication of an unstable system/network
      # and it could result in delayed or conflicting decisions on separate sides of a network
      # partition.
      # As a precaution for that scenario all nodes are downed if no decision is made within
      # `stable-after + down-all-when-unstable` from the first unreachability event.
      # The measurement is reset if all unreachable have been healed, downed or removed, or
      # if there are no changes within `stable-after * 2`.
      # The value can be on, off, or a duration.
      # By default it is 'on' and then it is derived to be 3/4 of stable-after.
      down-all-when-unstable = on

    }

    #OldestAutoDowning automatically downs unreachable members. A node responsible to down is the oldest member of a specified role.
    # If oldest-member-role is not specified, the oldest member among all cluster members fulfills its duty.
    #Downsides: all but one go down when the oldest is separated from the rest of the cluster
    #downing-provider-class = "org.sisioh.akka.cluster.custom.downing.OldestAutoDowning"

    #custom-downing {
    # Time margin after which shards or singletons that belonged to a downed/removed
    # partition are created in surviving partition. The purpose of this margin is that
    # in case of a network partition the persistent actors in the non-surviving partitions
    # must be stopped before corresponding persistent actors are started somewhere else.
    # This is useful if you implement downing strategies that handle network partitions,
    # e.g. by keeping the larger side of the partition and shutting down the smaller side.
    # Decision is taken by the strategy when there has been no membership or
    # reachability changes for this duration, i.e. the cluster state is stable.

    #stable-after = 5s

    #oldest-auto-downing {
    #oldest-member-role = ""

    #Downside of the oldest based downing strategy is loss of downing functionality when the oldest member itself fails.
    #If down-if-alone is set to be true, such scenario can be avoided because the secondary oldest member will down the oldest member
    #if the oldest member get unreachable alone.

    #down-if-alone = true
    #}
    #}
  }

  # CoordinatedShutdown will run the tasks that are added to these
  # phases. The phases can be ordered as a DAG by defining the
  # dependencies between the phases.
  # Each phase is defined as a named config section with the
  # following optional properties:
  # - timeout=15s: Override the default-phase-timeout for this phase.
  # - recover=off: If the phase fails the shutdown is aborted
  #                and depending phases will not be executed.
  # depends-on=[]: Run the phase after the given phases
  coordinated-shutdown {
    # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
    # if this is set to 'on'. It is done after termination of the
    # ActorSystem if terminate-actor-system=on, otherwise it is done
    # immediately when the last phase is reached.
    exit-jvm = on

    # Run the coordinated shutdown when ActorSystem.terminate is called.
    # Enabling this and disabling terminate-actor-system is not a supported
    # combination (will throw ConfigurationException at startup).
    run-by-actor-system-terminate = on


    default-phase-timeout = 10 seconds
  }

  #persistence {
  #  journal.plugin = "cassandra-journal"
  #  snapshot-store.plugin = "cassandra-snapshot-store"
  #}

  # https://doc.akka.io/docs/akka-persistence-cassandra/current/configuration.html#default-configuration
  persistence {

    #https://doc.akka.io/docs/akka/current/typed/persistence.html#replay-filter
    #https://blog.softwaremill.com/akka-cluster-split-brain-failures-are-you-ready-for-it-d9406b97e099
    #So if a split brain produces more wrong events than window-size then your aggregate state will be corrupted
    journal-plugin-fallback.replay-filter.window-size = 150 #100

    journal {
      plugin = ${akka.persistence.cassandra.journal}
      auto-start-journals = [akka.persistence.cassandra.journal]
    }

    snapshot-store {
      plugin = ${akka.persistence.cassandra.snapshot}
      auto-start-snapshot-stores = [akka.persistence.cassandra.snapshot]
    }

    cassandra {

      journal {

        # don't do this in production, convenient for local example
        keyspace-autocreate = true
        tables-autocreate = true

        keyspace = "akka_msg"
        table = "msg_queue"

        # Maximum number of messages that will be batched when using `persistAsync`.
        # Also used as the max batch size for deletes.
        max-message-batch-size = 200

        # Target number of entries per partition (= columns per row).
        # Must not be changed after table creation (currently not checked).
        # This is "target" as AtomicWrites that span partition boundaries will result in bigger partitions to ensure atomicity.
        target-partition-size = 1024

        replication-factor = 1 #3

        support-all-persistence-ids = off
      }

      events-by-tag {
        # Enable/disable events by tag. If eventsByTag queries aren't required then this should be set to
        # false to avoid the overhead of maintaining the tag_views table.
        enabled = false
      }

      snapshot {

        # don't do this in production, convenient for local example
        keyspace-autocreate = true
        tables-autocreate = true

        keyspace = "akka_msg_snapshot"
        table = "msg_queue_snapshots"

        replication-factor = 1 #3
      }
    }
  }
}


datastax-java-driver {

  advanced {
    reconnect-on-init = true
    #auth-provider {
    #class = PlainTextAuthProvider
    #username = ...
    #password = ...
    #}
  }

  basic {
    #contact-points = [""]
    load-balancing-policy.local-datacenter = dc1
  }

  profiles {
    akka-persistence-cassandra-profile {
      basic.request {
        #ONE only for development
        consistency = ONE
      }
    }

    akka-persistence-cassandra-snapshot-profile {
      basic.request {
        #ONE only for development
        consistency = ONE
      }
    }
  }
}


akka.reliable-delivery {

  producer-controller {

    # To avoid head of line blocking from serialization and transfer
    # of large messages this can be enabled.
    # Large messages are chunked into pieces of the given size in bytes. The
    # chunked messages are sent separatetely and assembled on the consumer side.
    # Serialization and deserialization is performed by the ProducerController and
    # ConsumerController respectively instead of in the remote transport layer.
    chunk-large-messages = off

    event-sourced-durable-queue {
      # Max duration for the exponential backoff for persist failures.
      restart-max-backoff = 10s

      # Snapshot after this number of events. See RetentionCriteria.
      snapshot-every = 128 #${akka.persistence.cassandra.journal.target-partition-size}

      # Number of snapshots to keep. See RetentionCriteria.
      keep-n-snapshots = 2

      # Delete events after snapshotting. See RetentionCriteria.
      delete-events = on

      # Cleanup entries that haven't be used for this duration.
      cleanup-unused-after = 3600 s

      # The journal plugin to use, by default it will use the plugin configured by `akka.persistence.journal.plugin`.
      journal-plugin-id = akka.persistence.cassandra.journal

      # The journal plugin to use, by default it will use the plugin configured by `akka.persistence.snapshot-store.plugin`.
      snapshot-plugin-id = akka.persistence.cassandra.snapshot
    }

    durable-queue {
      # The ProducerController uses this timeout for the requests to
      # the durable queue. If there is no reply within the timeout it
      # will be retried.
      request-timeout = 3s

      # The ProducerController retries requests to the durable queue this
      # number of times before failing.
      retry-attempts = 10000000

      # The ProducerController retries sending the first message with this interval
      # until it has been confirmed.
      resend-first-interval = 1s
    }
  }

  consumer-controller {
    # Number of messages in flight between ProducerController and
    # ConsumerController. The ConsumerController requests for more messages
    # when half of the window has been used.
    flow-control-window = 16 #16

    # The ConsumerController resends flow control messages to the
    # ProducerController with the resend-interval-min, and increasing
    # it gradually to resend-interval-max when idle.
    resend-interval-min = 2s
    resend-interval-max = 3s

    # If this is enabled lost messages will not be resent, but flow control is used.
    # This can be more efficient since messages don't have to be
    # kept in memory in the `ProducerController` until they have been
    # confirmed, but the drawback is that lost messages will not be delivered.
    only-flow-control = false
  }

  work-pulling {
    producer-controller = ${akka.reliable-delivery.producer-controller}
    producer-controller {
      # Limit of how many messages that can be buffered when there
      # is no demand from the consumer side.
      buffer-size = 32 #should be greater then consumer-controller.flow-control-window

      # Ask timeout for sending message to worker until receiving Ack from worker
      internal-ask-timeout = 60s

      # if (producerControllerSettings.chunkLargeMessagesBytes > 0) throw new IllegalArgumentException("Chunked messages not implemented for work-pulling yet.")

      # Chunked messages not implemented for work-pulling yet. Override to not
      # propagate property from akka.reliable-delivery.producer-controller.
      chunk-large-messages = off
    }
  }
}

datastax-java-driver2 {

  advanced {

    reconnect-on-init = true

    #auth-provider {
    #  class = PlainTextAuthProvider
    #  username = ...
    #  password = ...
    #}
  }

  basic {
    #contact-points = [""]
    load-balancing-policy.local-datacenter = "dc1"
  }

  profiles {
    akka-persistence-cassandra-profile {
      basic.request {
        #only for development
        consistency = ONE #QUORUM
      }
    }

    akka-persistence-cassandra-snapshot-profile {
      basic.request {
        #only for development
        consistency = ONE #QUORUM
      }
    }
  }
}

datastax-java-driver {

  profiles {

    profiles {
      akka-persistence-cassandra-profile {
        basic.request {
          #only for development
          consistency = ONE #QUORUM
        }
      }

      akka-persistence-cassandra-snapshot-profile {
        basic.request {
          #only for development
          consistency = ONE #QUORUM
        }
      }
    }

    local {

      basic {
        request.timeout = 3 seconds
        request.consistency = ONE

        contact-points = ["127.0.0.1:9042"]

        session-keyspace = chat

        load-balancing-policy {
          local-datacenter = dc1
        }
      }

    }
    cloud {
      basic.request.timeout = 3 seconds
      basic.request.consistency = QUORUM
    }
  }


  #need in LimitConcurrencyRequestThrottler example
  #advanced.throttler {
  #  class = ConcurrencyLimitingRequestThrottler
  #  max-concurrent-requests = ${cassandra.parallelism}
  #  max-queue-size = 128
  #}

  advanced {
    request.warn-if-set-keyspace = false

    #defauls
    #https://docs.datastax.com/en/developer/java-driver/4.17/manual/core/pooling/
    connection {
      max-requests-per-connection = 1024
      pool {
        local.size = 1
        remote.size = 1
      }
    }
  }
}


pinned-dispatcher {
  type = PinnedDispatcher
  executor = "thread-pool-executor"
}

fj-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-max = 8
  }
}


akka.management.http.port = 8558

akka.management.cluster.bootstrap.contact-point-discovery {
  service-name = work-pulling
  discovery-method = config

  # boostrap filters ports with the same IP assuming they are previous instances running on the same node
  # unless a port is specified
  port-name = "management"
  required-contact-point-nr = 1
  # config service discovery never changes
  stable-margin = 3 ms
  # bootstrap without all the nodes being up
  contact-with-all-contact-points = false
}

akka.discovery.config.services {
  "work-pulling" {
    endpoints = [
      {host = "127.0.0.1", port = 8558}
      {host = "127.0.0.2", port = 8558}
    ]
  }
}

num-of-workers=2